{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Assignment Code: DA-AG-012*\n",
        "*Decision Tree | Assignment*\n",
        "\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "-\tA decision tree is a decision support recursive partitioning structure that uses a tree-like model of decisions and their possible consequences.\n",
        "-\tDecision tree employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- Entropy measures a set's disorder level, while Gini impurity quantifies the probability of misclassifying instances. Both are used in decision trees to determine node splits, but Gini favors larger partitions.\n",
        "- In decision trees, the best split at each node is determined by evaluating how well each potential division separates the data, using criteria like Gini impurity, entropy. A lower Gini impurity suggests a more homogeneous set of elements within the node, making it an attractive split in a decision tree. At every branch, the entropy computed for the target column is the weighted entropy. The weighted entropy means taking the weights of each attribute. The weights are the probability of each of the classes. The more the decrease in the entropy, the more is the information gained.\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- The pre-pruning or early stopping involves stopping the tree before it has completed classifying the training set and post-pruning refers to pruning the tree after it has finished. Post-Pruning is used generally for small datasets whereas Pre-Pruning is used for larger ones.\n",
        "- Pre-pruning stops the tree from growing too large during training. Post-pruning, on the other hand, trims a fully grown tree by removing parts that don't improve accuracy. Both techniques aim to prevent overfitting, but they take different approaches to simplifying a decision tree.\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- Information gain is the basic criterion to decide whether a feature should be used to split a node or not.\n",
        "- The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node. The best split at each node is determined by evaluating how well each potential division separates the data, using criteria like Gini impurity, entropy, or sum of squared errors.\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "- The most commonly used applications of decision trees are data mining and data classification. It has a wide range of applications in various fields. They are used in medical research and practice for diagnostic testing and in the field of genomics. In business, decision trees are used for strategy formulation, decision making, and to visualize cost effectiveness.\n",
        "- The limitations are overfitting of data, lack of robustness (highly sensitive to small variations in the data), limitations in handling imbalanced data (Decision trees tend to favor the majority class during splits, leading to biased predictions and poor model performance in minority classes), difficulty in modeling interactions, Scalability Concerns.\n",
        "- The advantages are easy to interpret, little or no data preparation required, more flexible for both classification and regression tasks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h2lQ-e-Od7xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\"\"\"\n"
      ],
      "metadata": {
        "id": "v0XytG6mD0Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load Iris dataset into a pandas DataFrame\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Split the data\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy and confusion matrix\n",
        "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Feature importances as a DataFrame\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0aH8hyyeNmZ",
        "outputId": "c28b2f9a-29ec-4dcc-a306-cc212701b882"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=25)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train fully grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(f\"Decision Tree (max_depth=3) Accuracy: {accuracy_limited:.2f}\")\n",
        "print(f\"Decision Tree (full tree) Accuracy: {accuracy_full:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfWqxCxFeNqV",
        "outputId": "742f9e96-4254-4499-eaac-c335b32448eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree (max_depth=3) Accuracy: 1.00\n",
            "Decision Tree (full tree) Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Display feature importances\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": regressor.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv0fLGy6eNt3",
        "outputId": "fc395a70-fc4f-4572-f423-2ba660414d82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.42\n",
            "\n",
            "Feature Importances:\n",
            "    Feature  Importance\n",
            "5        RM    0.600326\n",
            "12    LSTAT    0.193328\n",
            "7       DIS    0.070688\n",
            "0      CRIM    0.051296\n",
            "4       NOX    0.027148\n",
            "6       AGE    0.013617\n",
            "9       TAX    0.012464\n",
            "10  PTRATIO    0.011012\n",
            "11        B    0.009009\n",
            "2     INDUS    0.005816\n",
            "1        ZN    0.003353\n",
            "8       RAD    0.001941\n",
            "3      CHAS    0.000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Decision Tree model\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]}\n",
        "\n",
        "# 5. Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dtree,param_grid=param_grid,cv=5,scoring='accuracy',n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best model and parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# 7. Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 8. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 9. Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2WAFeDZeNze",
        "outputId": "e55fa302-3001-49ca-ac23-eb6f75813906"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.'''\n",
        "\n",
        "#1. Handle Missing Values\n",
        "\n",
        "'''Missing data is common in healthcare (e.g., lab tests not taken, incomplete forms).\n",
        "\n",
        "Steps:\n",
        "\n",
        "Identify missing values:'''\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "#2. Handle Missing Values\n",
        "\n",
        "'''For numbers: Fill missing values with the median (better for skewed data).\n",
        "\n",
        "For categories: Fill missing values with the most common value or label them as \"Unknown\".'''\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# 3. Encode Categorical Features\n",
        "\n",
        "# 4. Train a Decision Tree Model\n",
        "\n",
        "'''Split data into training and testing sets.\n",
        "\n",
        "Fit the model on processed data.'''\n",
        "\n",
        "# 5. Tune Hyperparameters with GridSearchCV\n",
        "\n",
        "'''Key hyperparameters:\n",
        "\n",
        "max_depth: Controls tree depth\n",
        "\n",
        "min_samples_split: Minimum samples required to split a node\n",
        "\n",
        "min_samples_leaf: Minimum samples at leaf nodes\n",
        "\n",
        "criterion: gini or entropy'''\n",
        "\n",
        "# 6. Evaluate Performance\n",
        "\n",
        "'''Use both training and testing data to assess performance.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Accuracy – Overall correctness\n",
        "\n",
        "Precision & Recall – Important in healthcare where false negatives/positives matter\n",
        "\n",
        "F1-score – Balance between precision and recall\n",
        "\n",
        "ROC-AUC – Probability that model ranks positive higher than negative'''\n",
        "\n",
        "# 7.Business Value\n",
        "\n",
        "'''A well-trained Decision Tree model can:\n",
        "\n",
        "Support doctors: Provide quick, data-driven insights to flag high-risk patients.\n",
        "\n",
        "Improve early detection: Catch diseases early, improving patient outcomes.\n",
        "\n",
        "Optimize resources: Focus diagnostic tests or treatments on high-probability cases.\n",
        "\n",
        "Reduce costs: Prevent unnecessary tests and hospitalizations.\n",
        "\n",
        "Compliance & Explainability: Decision Trees are interpretable, helping meet regulatory requirements in healthcare.'''\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w1gs-MAPeN2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}